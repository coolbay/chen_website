---
title: "Owl (observe, watch, listen): Localizing actions in egocentric video via audiovisual temporal context"
publication_types:
  - "3"
authors:
  - Merey Ramazanova
  - Victor Escorcia
  - Fabian Caba Heilbron
  - admin
  - Bernard Ghanem

publication_short: arXiv'22
abstract: Temporal action localization (TAL) is an important task extensively explored and improved for third-person videos in recent years. Recent efforts have been made to perform fine-grained temporal localization on first-person videos. However, current TAL methods only use visual signals, neglecting the audio modality that exists in most videos and that shows meaningful action information in egocentric videos. In this work, we take a deep look into the effectiveness of audio in detecting actions in egocentric videos and introduce a simple-yet-effective approach via Observing, Watching, and Listening (OWL) to leverage audio-visual information and context for egocentric TAL. For doing that, we 1) compare and study different strategies for where and how to fuse the two modalities; 2) propose a transformer-based model to incorporate temporal audio-visual context. Our experiments show that our approach achieves state-of-the-art performance on EPIC-KITCHENS-100.
draft: false
featured: false
tags:
  - Deep learning
  - Multi-modality fusion
  - Temporal action detection
  - Transformers
slides: ""
url_pdf: https://arxiv.org/pdf/2202.04947.pdf
image:
  caption: ""
  focal_point: ""
  preview_only: false
  filename: ""
url_dataset: ""
url_project: ""
url_source: ""
url_video: ""
author_notes: []
doi: ""
publication: arXiv preprint, 2022
projects: []
date: 2022-01-01T00:00:02.020Z
url_slides: ""
publishDate: 2022
url_poster: ""
url_code: ""
---
